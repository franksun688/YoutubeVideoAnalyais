{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import nltk\n",
    "\n",
    "import spacy\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "import wordcloud\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "nlp = spacy.load('en_core_web_sm', parse = True, tag=True, entity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions:\n",
    "Subscriber: 'k'/'m' to numbers <br>\n",
    "View: 'k'/'m' to numbers <br>\n",
    "Uploadtime: 'hours/days/weeks/months/years ago' to number of hours <br>\n",
    "Length: to total seconds <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_document(doc):\n",
    "    \"\"\"\n",
    "    This function takes string as argument and output cleaned texts for further analysis\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    # remove accented characters\n",
    "    doc = unicodedata.normalize('NFKD', doc).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "    # initiate lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # lemmatizing\n",
    "    \n",
    "    #doc = lemmatize_text(tokens)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # re-create document from filtered tokens, lemmatized\n",
    "    doc = ' '.join([lemmatizer.lemmatize(w,pos='n') for w in filtered_tokens])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    \"\"\"\n",
    "    This function takes in a pandas data set, \n",
    "    drops missing values, clean the text, \n",
    "    transfers subscribers, views and time after uploads from string to numbers\n",
    "    and output a new data frame.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    data = data.dropna()\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    data.subscriber = data.subscriber.replace(r'[KM]+$', '', regex=True).astype(float)*\\\n",
    "                data.subscriber.str.extract(r'[\\d\\.]+([KM]+)', expand=False).fillna(1).replace(['K','M'], [10**3, 10**6])\n",
    "    data.subscriber = data.subscriber.astype(int)\n",
    "    \n",
    "    data.view = data.view.str.replace('views', '')\n",
    "    data.view = data.view.str.replace(' ', '')\n",
    "    data.view = data.view.replace(r'[KM]+$', '', regex=True).astype(float)*\\\n",
    "                data.view.str.extract(r'[\\d\\.]+([KM]+)', expand=False).fillna(1).replace(['K','M'], [10**3, 10**6])\n",
    "    data.view = data.view.astype(int)\n",
    "    \n",
    "    data.uploadtime = data.uploadtime.str.replace('Streamed', ''); data.uploadtime = data.uploadtime.str.replace('ago', '')\n",
    "    data.uploadtime = data.uploadtime.str.replace('hours','H'); data.uploadtime = data.uploadtime.str.replace('hour','H')\n",
    "    data.uploadtime = data.uploadtime.str.replace('days','D'); data.uploadtime = data.uploadtime.str.replace('day','D')\n",
    "    data.uploadtime = data.uploadtime.str.replace('weeks','W'); data.uploadtime = data.uploadtime.str.replace('week','W')\n",
    "    data.uploadtime = data.uploadtime.str.replace('months','M'); data.uploadtime = data.uploadtime.str.replace('month','M')\n",
    "    data.uploadtime = data.uploadtime.str.replace('years','Y'); data.uploadtime = data.uploadtime.str.replace('year','Y')\n",
    "    data.uploadtime = data.uploadtime.str.replace(' ','')\n",
    "    \n",
    "    data.uploadtime = data.uploadtime.replace(r'[HDWMY]+$', '', regex=True).astype(float)*\\\n",
    "                data.uploadtime.str.extract(r'[\\d\\.]+([HDWMY]+)', expand=False).fillna(1).replace(['H','D','W','M','Y'], [1,24,7*24,30*24,365*24])\n",
    "    data.uploadtime = data.uploadtime.astype(int)\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "    for i in range(data.shape[0]):\n",
    "        if len(data.length[i])==4: data.length[i] = '00:0'+data.length[i]\n",
    "        if len(data.length[i])==5: data.length[i] = '00:'+data.length[i]\n",
    "\n",
    "    data.length = pd.to_timedelta(data.length).dt.total_seconds().astype(int)\n",
    "\n",
    "    data.like = data.like.astype(int)\n",
    "    data.dislike = data.dislike.astype(int)\n",
    "    \n",
    "    normalize_corpus = np.vectorize(normalize_document)\n",
    "    data['clean_text'] = normalize_corpus(data['title'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and Clean our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(r'data/video_03102020.csv', index_col = 0, thousands = ',', encoding = 'utf-8')\n",
    "data = clean(data)\n",
    "data.to_csv('data/video_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = pd.read_csv(r'data/video_03142020_143324.csv', index_col = 0, thousands = ',', encoding = 'utf-8')\n",
    "data2 = clean(data2)\n",
    "data2.to_csv('data/video_clean2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
