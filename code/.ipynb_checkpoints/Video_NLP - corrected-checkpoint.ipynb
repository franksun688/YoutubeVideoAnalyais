{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean raw search results from web scraping\n",
    "# input: raw data\n",
    "# output: cleaned data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from collections import Counter\n",
    "import wordcloud\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-e30bf9010aa6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#nltk.download('wordnet')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'en_core_web_sm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/spacy/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdepr_path\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mdeprecation_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW001\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdepr_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(name, **overrides)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exists\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Path or Path-like to model data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_model_from_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moverrides\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a shortcut link, a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "\n",
    "#nltk.download('wordnet')\n",
    "nlp = spacy.load('en_core_web_sm', parse = True, tag=True, entity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions:\n",
    "Subscriber: 'k'/'m' to numbers <br>\n",
    "View: 'k'/'m' to numbers <br>\n",
    "Uploadtime: 'hours/days/weeks/months/years ago' to number of hours <br>\n",
    "Length: to total seconds <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_document(doc):\n",
    "    \"\"\"\n",
    "    This function takes string as argument and output cleaned texts for further analysis\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    \n",
    "    # lower case and remove special characters\\whitespaces\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc, re.I|re.A)\n",
    "    doc = doc.lower()\n",
    "    doc = doc.strip()\n",
    "    \n",
    "    # remove accented characters\n",
    "    doc = unicodedata.normalize('NFKD', doc).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    \n",
    "    # initiate lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    # tokenize document\n",
    "    tokens = wpt.tokenize(doc)\n",
    "    # lemmatizing\n",
    "    \n",
    "    #doc = lemmatize_text(tokens)\n",
    "    # filter stopwords out of document\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    \n",
    "    # re-create document from filtered tokens, lemmatized\n",
    "    doc = ' '.join([lemmatizer.lemmatize(w,pos='n') for w in filtered_tokens])\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    \"\"\"\n",
    "    This function takes in a pandas data set, \n",
    "    drops missing values, clean the text, \n",
    "    transfers subscribers, views and time after uploads from string to numbers\n",
    "    and output a new data frame.\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    data = data.dropna()\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    data.subscriber = data.subscriber.replace(r'[KM]+$', '', regex=True).astype(float)*\\\n",
    "                data.subscriber.str.extract(r'[\\d\\.]+([KM]+)', expand=False).fillna(1).replace(['K','M'], [10**3, 10**6])\n",
    "    data.subscriber = data.subscriber.astype(int)\n",
    "    \n",
    "    data.view = data.view.str.replace('views', '')\n",
    "    data.view = data.view.str.replace(' ', '')\n",
    "    data.view = data.view.replace(r'[KM]+$', '', regex=True).astype(float)*\\\n",
    "                data.view.str.extract(r'[\\d\\.]+([KM]+)', expand=False).fillna(1).replace(['K','M'], [10**3, 10**6])\n",
    "    data.view = data.view.astype(int)\n",
    "    \n",
    "    data.uploadtime = data.uploadtime.str.replace('Streamed', ''); data.uploadtime = data.uploadtime.str.replace('ago', '')\n",
    "    data.uploadtime = data.uploadtime.str.replace('hours','H'); data.uploadtime = data.uploadtime.str.replace('hour','H')\n",
    "    data.uploadtime = data.uploadtime.str.replace('days','D'); data.uploadtime = data.uploadtime.str.replace('day','D')\n",
    "    data.uploadtime = data.uploadtime.str.replace('weeks','W'); data.uploadtime = data.uploadtime.str.replace('week','W')\n",
    "    data.uploadtime = data.uploadtime.str.replace('months','M'); data.uploadtime = data.uploadtime.str.replace('month','M')\n",
    "    data.uploadtime = data.uploadtime.str.replace('years','Y'); data.uploadtime = data.uploadtime.str.replace('year','Y')\n",
    "    data.uploadtime = data.uploadtime.str.replace(' ','')\n",
    "    \n",
    "    data.uploadtime = data.uploadtime.replace(r'[HDWMY]+$', '', regex=True).astype(float)*\\\n",
    "                data.uploadtime.str.extract(r'[\\d\\.]+([HDWMY]+)', expand=False).fillna(1).replace(['H','D','W','M','Y'], [1,24,7*24,30*24,365*24])\n",
    "    data.uploadtime = data.uploadtime.astype(int)\n",
    "    \n",
    "    warnings.filterwarnings('ignore')\n",
    "    for i in range(data.shape[0]):\n",
    "        if len(data.length[i])==4: data.length[i] = '00:0'+data.length[i]\n",
    "        if len(data.length[i])==5: data.length[i] = '00:'+data.length[i]\n",
    "\n",
    "    data.length = pd.to_timedelta(data.length).dt.total_seconds().astype(int)\n",
    "\n",
    "    data.like = data.like.astype(int)\n",
    "    data.dislike = data.dislike.astype(int)\n",
    "    \n",
    "    normalize_corpus = np.vectorize(normalize_document)\n",
    "    data['clean_text'] = normalize_corpus(data['title'])\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import and Clean first search result and most recent search result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'../data/video_***.csv'\n",
    "\n",
    "data = pd.read_csv(r'data/video_03102020.csv', index_col = 0, thousands = ',', encoding = 'utf-8')\n",
    "data = clean(data)\n",
    "data.to_csv('data/video_clean.csv')\n",
    "\n",
    "data2 = pd.read_csv(r'data/video_03142020_143324.csv', index_col = 0, thousands = ',', encoding = 'utf-8')\n",
    "data2 = clean(data2)\n",
    "data2.to_csv('data/video_clean2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### clean all of the search raw results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['video_03142020_211654',\n",
       " 'video_03142020_011950',\n",
       " 'video_03132020_120531',\n",
       " 'video_03132020_084652',\n",
       " 'video_03152020_035739',\n",
       " 'video_03142020_143324',\n",
       " 'video_03122020_162930',\n",
       " 'video_03132020_020920',\n",
       " 'video_03132020_052811',\n",
       " 'video_03132020_152345',\n",
       " 'video_03142020_043854',\n",
       " 'video_03142020_175505',\n",
       " 'video_03142020_075258',\n",
       " 'video_03122020_194857',\n",
       " 'video_03122020_230910',\n",
       " 'video_03132020_220046',\n",
       " 'video_03122020_131055',\n",
       " 'video_03152020_003817',\n",
       " 'video_03142020_111230',\n",
       " 'video_03132020_184202']"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames = glob.glob(\"../rawdata/*.csv\")\n",
    "files = [i.split('/')[2] for i in filenames]\n",
    "filenamelist = [i.split('.')[0] for i in files]\n",
    "filenamelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video_03132020_152345\n",
      "video_03122020_230910\n",
      "video_03142020_111230\n"
     ]
    }
   ],
   "source": [
    "for i in filenamelist:\n",
    "    path = '../rawdata/' + i + '.csv'\n",
    "    data = pd.read_csv(path, index_col = 0, thousands = ',', encoding = 'utf-8')\n",
    "    try:\n",
    "        data = clean(data)\n",
    "        name = '../data/clean/' + i + '_clean.csv'\n",
    "        data.to_csv(name)\n",
    "    except ValueError:\n",
    "        print(i)\n",
    "        print"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
